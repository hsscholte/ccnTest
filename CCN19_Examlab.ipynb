{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCN19_Examlab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsscholte/ccnTest/blob/master/CCN19_Examlab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuS_zgFIgPX3",
        "colab_type": "text"
      },
      "source": [
        "## Learning to remember: T-Maze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9CVMUZhjNn",
        "colab_type": "text"
      },
      "source": [
        "Many many behavioral tasks require that some part of past information is kept in memory, to make the correct decision at a later time. Think of reading a room number for a lecture, and keeping it in mind until you have reached the specific room.  \n",
        "\n",
        "When we give such tasks to monkeys, we find that specific subsets of neurons will exhibit persistent activity from seeing the object to be remembered to the end of the trial. This is believed to reflect some aspect of working memory. \n",
        "\n",
        "In behavioral science, a T-maze (or the variant Y-maze) is a simple maze used in animal cognition experiments. It is shaped like the letter T (or Y), providing the subject, typically a rodent, with a straightforward choice. T-mazes are used to study how the rodents function with memory and spatial learning through applying various stimuli. Starting in the early 20th century, rodents were used in experiments such as the T-Maze. These concepts of T-mazes are used to assess rodent behavior. The different tasks, such as left-right discrimination and forced alternation, are mainly used with rodents to test reference and working memory.\n",
        "\n",
        "Here, we study how a T-Maze task can be learned thorugh reinforcement learning with a recurrent neural network to *learn to* remember the correct information in a task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaZFmIHjg_qy",
        "colab_type": "text"
      },
      "source": [
        "## Recalling the LSTM: Long Short-Term Memory\n",
        "\n",
        "Here, we give a simple example of a recurrent neural network specifically designed for learning to keep information active: the LSTM unit, where LSTM stands for Long Short-Term Memory. The LSTM was invented by Sepp Hochreiter and Juergen Schmidhuber, back in 1997, and addresses specific needs to make learning memory in neural networks feasible. The key unit in the LSTM is a neuron with a self-recurrent connection; through this self-recurrency, past information is maintained as persistent (or decaying) activity in the unit. \n",
        "\n",
        "With the LSTM, at every timestep, an input $x_t$ is put into the neural unit, and this input affects the internal state of the unit in three ways: on forgetting the internal state, on the value of the internal state, and on the output of the internal state. An LSTM unit thus has a *forget gate*, an *input gate*, and an *output gate*. Can you see the gates in this figure? Hint: the top horizontal line corresponds to the internal state. \n",
        "\n",
        "![LSTM](https://drive.google.com/thumbnail?id=1rrUy5QUIn4WhA_2tYZMeZ64DFKuCpkLt&sz=w600)\n",
        "\n",
        "From left to right, the new input $x_t$ together with the output from the unit at the previous state $h_{t-1}$ acts on the internal state with a multiplication. This effectively is the *forget gate*. The next to blocks ($\\sigma$ and `tanh`) determine how much of a function of the input is added to the internal state, through an *input gate*, and the last sigma block determines how much output is gated by the present input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqV6GTbVhGVd",
        "colab_type": "text"
      },
      "source": [
        "## The Working-Memory T-Maze task\n",
        "\n",
        "The T-Maze is a working memory task adapted from the one used in (Bakker, 2001). The agent has to select to move *Left* or *Right* at the end of a corridor, accordingly to the information given at the beginning of the task -- a road-sign (which could be at either the right or left side at the start of the corridor). The agent had to remember this information to select the optimal action among the four possible: *Up, Down, Left, Right*. The length of the corridor $N$ can be varied, and the agent moves with a step-size of 1. A movement through the wall returned a negative reward of $r_{wall} = -0.1$. At the T-junction, if the agent moved correctly it was rewarded with $r_{goal} = 4$, while a incorrect choice returned $r_{wrong} = -0.1$. The learning was stopped when the agent made $90\\%$ correct choices, for each condition, in the last $50$ trials. \n",
        "\n",
        "![T-Maze](https://drive.google.com/thumbnail?id=1xHLjEg76NwtrhxapoIGs2wF3q_FPfiMf&sz=w600)\n",
        "\n",
        "The T-Maze can be used to illustrate the need for gating in Working Memory: when we add noise to the representation of the maze, the agent needs to actively protect its memory of the road sign from deteriorating due to the variable content of the observations $X_t$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiBR6OObhOkv",
        "colab_type": "text"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "The T-Maze tasks uses an agent that receives as input an abstract representation of its present location in the T-Maze. So mostly, that is the corridor which is identical at every step.\n",
        "\n",
        "The network consists of three input neurons, describing the current observations in terms of what the agent can see to the left, front and right. There is a single hidden layer with (here) 12 \"normal\" neurons and 3 LSTM units. These 15 units project to 4 output neurons corresponding to the four possible actions that the agent can take, *Up, Down, Left, Right*. \n",
        "\n",
        "![NetworkLSTM](https://drive.google.com/thumbnail?id=1BE7r1ewJflWdeq1zqEFmGU1kr1nPL-xG&sz=w400)\n",
        "\n",
        "The output of each output neuron is interpreted as a Q-value, the aim of the network is to learn the action sequence that maximizes the total reward. For this, it needs to learn to remember the road sign, move straight along the corridor, and turn left or right - depending on the road sign - at the end of the corridor when it observes that there are no walls to the left or right but there is a wall in front of the agent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXnVIaMADQsX",
        "colab_type": "text"
      },
      "source": [
        "## The agent\n",
        "\n",
        "We put together an working memory agent as described above.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxz7vTnygZHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if os.path.exists('TMaze_main_tutorial_2019.py') != True:\n",
        "  ! wget https://github.com/isapome/teaching/raw/master/TMaze_main_tutorial_2019.py\n",
        "if os.path.exists('TMaze_task_tutorial_2019.py') != True:\n",
        "  ! wget https://github.com/isapome/teaching/raw/master/TMaze_task_tutorial_2019.py\n",
        "if os.path.exists('weights_trial_10_noiseless.pkl') != True:\n",
        "  ! wget https://github.com/isapome/teaching/raw/master/weights_trial_10_noiseless.pkl\n",
        "if os.path.exists('weights_trial_10_noisy.pkl') != True:\n",
        "  ! wget https://github.com/isapome/teaching/raw/master/weights_trial_10_noisy.pkl\n",
        "if os.path.exists('weights_trial_20_noiseless.pkl') != True:\n",
        "  ! wget https://github.com/isapome/teaching/raw/master/weights_trial_20_noiseless.pkl\n",
        "\n",
        "import TMaze_task_tutorial_2019 as TMaze\n",
        "from TMaze_task_tutorial_2019 import TaskTmaze\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kLmFB8fDn4V",
        "colab_type": "text"
      },
      "source": [
        "The code below allows you to run the agent in various modes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtPFcHPck5uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_qvalues(predictions, save_name=None):\n",
        "    ax = predictions.plot(lw = 2)\n",
        "    ax.set_ylabel('Q-values', fontsize=18)\n",
        "    ax.set_xlabel('Action #', fontsize=18)\n",
        "    ax.tick_params(labelsize=14)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "           ncol=4, mode=\"expand\", borderaxespad=0., fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    if save_name:\n",
        "        plt.savefig(save_name, bbox_inches='tight') \n",
        "    return\n",
        "\n",
        "def experiment(mode, save_weights=False, filename=None):\n",
        "    if noise:\n",
        "        str_noise = 'noisy'\n",
        "    else:\n",
        "        str_noise = 'noiseless'\n",
        "    print(\"TMaze \"+str_noise+\" experiment. Corridor length: \", corridor_length, \"\\n\")\n",
        "    if filename:\n",
        "        if filename[-4:]!= '.pkl':\n",
        "            raise ValueError('The filename needs to end with .pkl') \n",
        "    if mode=='train':\n",
        "        print(\"Training process:\")\n",
        "        weights = TMaze.start_training(seed, corridor_length, learning_rate, n_trainings, discount_factor, noise=noise)\n",
        "        if save_weights == True:\n",
        "            if filename:\n",
        "                with open(filename, 'wb') as file:\n",
        "                    pickle.dump(weights, file)\n",
        "            else:\n",
        "                raise ValueError('For save mode the name of the weight file needs to be provided.')  \n",
        "    elif mode=='load':\n",
        "        print(\"Loading the weights...\")\n",
        "        if filename:\n",
        "            with open(filename, 'rb') as file:\n",
        "                weights = pickle.load(file, encoding='latin1')\n",
        "                print(\"...weights loaded.\")\n",
        "        else:\n",
        "            raise ValueError('For load mode the name of the weight file needs to be provided.') \n",
        "    else:\n",
        "        raise ValueError('Unknown mode. Choose \\'train\\' or \\'load\\'.')\n",
        "        \n",
        "    print(\" \")\n",
        "    print(\"Testing the network:\")\n",
        "    Results = {}\n",
        "    # a number <0.5 sets the road sign on the left, a number >= sets the road sign to the right\n",
        "    for i in [0.1,  0.9]:\n",
        "        print( \" \")\n",
        "        results = TMaze.testing(weights, i, corridor_length, noise=noise)\n",
        "        plot_qvalues(results['predictions'])\n",
        "        Results['results'+str(i)] = results\n",
        "    return Results\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP3Q5SKJH09N",
        "colab_type": "text"
      },
      "source": [
        "### Task\n",
        "\n",
        "<font color=blue>\n",
        "Given the seed you received, compare three different combinations of learning rate and discount factor and report your findings (without noise!). Which combination works best? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEIDkzo8jjVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to train a network with a set of hyper-parameters. \n",
        "#----Train network and save weights (if save_weights==True)\n",
        "seed = 1234 # enter the seed that will be handed out to you\n",
        "corridor_length = 12\n",
        "learning_rate = 0.0005\n",
        "discount_factor = -0.4\n",
        "noise = None #choose None for noiseless, anything else for noisy\n",
        "n_trainings = 50000 \n",
        "weights_filename = 'MYweights_tutorial.pkl'\n",
        "results = experiment('train', save_weights=False, filename=weights_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}